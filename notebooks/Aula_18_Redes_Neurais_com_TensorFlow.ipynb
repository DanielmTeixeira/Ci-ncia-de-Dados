{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/storopoli/ciencia-de-dados/master?filepath=notebooks%2FAula_18_Redes_Neurais_com_TensorFlow.ipynb)\n",
    "<br>\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/storopoli/ciencia-de-dados/blob/master/notebooks/Aula_18_Redes_Neurais_com_TensorFlow.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Redes Neurais com TensorFlow\n",
    "\n",
    "**Objetivos**: Aprender Redes Neurais Artificiais (RNA) usando a biblioteca `TensorFlow`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TensorFlow e Keras\n",
    "\n",
    "[**TensorFlow**](https://www.tensorflow.org/) é uma biblioteca de código aberto para aprendizado de máquina aplicável a uma ampla variedade de tarefas. Foi criada pelo **Google** em 2015  é a principal biblioteca para criação e treinamento de redes neurais artificiais. A API toda é escrita em Python mas é executada em C++ na CPU ou em CUDA na GPU.\n",
    "\n",
    "[**Keras**](https://keras.io/) é uma biblioteca de código aberto para fácil criação e treinamento de redes neurais artificiais. Projetado para permitir experimentação rápida com redes neurais profundas, ele se concentra em ser fácil de usar, modular e extensível. Em 2017, a equipe do **TensorFlow** decidiu apoiar o Keras na biblioteca principal do **TensorFlow**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## O que é uma Rede Neural Artificial?\n",
    "\n",
    "Redes neurais artificiais (RNAs) são modelos computacionais inspirados pelo sistema nervoso central (em particular o cérebro) que são capazes de realizar o aprendizado de máquina bem como o reconhecimento de padrões. Redes neurais artificiais geralmente são apresentadas como sistemas de \"neurônios interconectados, que podem computar valores de entradas\", simulando o comportamento de redes neurais biológicas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/redes_neurais.jpeg\" alt=\"redes neurais\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Como a rede neural aprende?\n",
    "\n",
    "Em cada neurônio há uma função de ativação (*activation function*) que processa uma combinação linear entre inputs e pesos sinápticos, gerando assim um sinal de saída.\n",
    "\n",
    "A informação flui da *input layer* para as *hidden layers* e por fim para a *output layer*. Nesse fluxo os inputs de dados da *input layer* são alimentados para os neurônios das *hidden layers* que por fim alimentam o neurônio final da *output layer*.\n",
    "\n",
    "A primeira passada de informação (propagação) pela rede é geralmente feita com parâmetros aleatórios para as funções de ativação dos neurônios.\n",
    "\n",
    "Ao realizar a propagação, chamada de *feed forward*, temos sinais de saídas nos neurônios da output layer. \n",
    "\n",
    "No fim da propagação, a função custo (uma métrica de erro) é calculada e o modelo então ajusta os parâmetros dos neurônios na direção de um menor custo (por meio do gradiente - derivada multivariada).\n",
    "\n",
    "Assim uma nova propagação é gerada e a numa nova função custo e calculada. Assim como é realizado a atualização dos parâmetros dos neurônios.\n",
    "\n",
    "O nome desse algoritmo é **Retro-propagação** (*Backpropagation*). E cada vez que ele é executado denomina-se como época (*epoch*). E quandos as épocas estabelecidas se encerram, a rede neural encerra o seu treinamento/aprendizagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/backpropagation.gif\" alt=\"backpropagation\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Funções de Ativação\n",
    "\n",
    "| **Sigmoid**                                                  | **Tanh**                                                     | **ReLU**                                                     | **Leaky ReLU**                                               |\n",
    "| ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |\n",
    "| $g(z)=\\frac{1}{1+e^{-z}}$                                    | $g(z)=\\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}}$                     | $g(z)=\\max (0, z)$                                           | $\\begin{array}{c}{g(z)=\\max (\\epsilon z, z)} \\\\ {\\text { com } \\epsilon \\ll 1}\\end{array}$ |\n",
    "| ![Illustration](https://cdn.jsdelivr.net/gh/storopoli/notes-pictures@master/uPic/sigmoid.png) | ![Illustration](https://cdn.jsdelivr.net/gh/storopoli/notes-pictures@master/uPic/tanh.png) | ![Illustration](https://cdn.jsdelivr.net/gh/storopoli/notes-pictures@master/uPic/relu.png) | ![Illustration](https://cdn.jsdelivr.net/gh/storopoli/notes-pictures@master/uPic/leaky-relu.png) |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Algoritmos de Otimização\n",
    "\n",
    "Keras possui diversos:\n",
    "* SGD\n",
    "* RMSprop\n",
    "* Adam\n",
    "* Adadelta\n",
    "* Adagrad\n",
    "* Adamax\n",
    "* Nadam\n",
    "* Ftrl\n",
    "\n",
    "Os mais importantes são o SGD e o Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SGD - Stochastic Gradient Descent\n",
    "\n",
    "[`tf.keras.optimizers.SGD()`](https://keras.io/api/optimizers/sgd/):\n",
    "\n",
    "* `learning_rate=0.01` - Taxa de Aprendizagem $\\eta$\n",
    "* `momentum=0.0` - hyperparâmetro $\\geq 0$ que acelera o *gradient descent* na direção relevante e mitiga oscilações. \n",
    "* `nesterov=False` - `bool` para se aplica *Nesterov Momentum* ou não. *Nesterov Momentum* usa posições intermediárias do gradiente no cálculo do *momentum*. Proposto por Yuri Nesterov em 1983.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Momentum\n",
    "\n",
    "<img src=\"images/momentum.gif\" alt=\"momentum\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adam\n",
    "\n",
    "Adam é um método de SGD que usa uma estimativa adaptativa dos momentos de primeira ordem e momentos de segunda ordem. Proposto por Kingma & Ba (2014).\n",
    "\n",
    "[`tf.keras.optimizers.Adam()`](https://keras.io/api/optimizers/adam/):\n",
    "\n",
    "* `learning_rate=0.001` - Taxa de Aprendizagem $\\eta$. Valor padrão menor que o do `SGD()`.\n",
    "* `beta_1=0.9` - O decrescimento exponencial da estimativa dos momentos de primeira ordem.\n",
    "* `beta_2=0.999` - O decrescimento exponencial da estimativa dos momentos de segunda ordem.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Momentum\n",
    "\n",
    "<img src=\"images/comparacao_otimizadores.gif\" alt=\"comparacao_otimizadores\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Funções Custo\n",
    "\n",
    "As funções custos se dividem em dois tipos:\n",
    "\n",
    "1. Funções Custo de **Classificação**\n",
    "2. Funções Custo de **Regressão**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Funcões Custo de Classificação\n",
    "\n",
    "Mais utilizadas\n",
    "\n",
    "\n",
    "* *Binary Cross-entropy* (Entropia cruzada binária): [`tf.keras.losses.BinaryCrossentropy()`](https://keras.io/api/losses/probabilistic_losses/#binarycrossentropy-class)\n",
    "* *Categorical Cross-entropy* (Entropia cruzada categórica): [`tf.keras.losses.CategoricalCrossentropy()`](https://keras.io/api/losses/probabilistic_losses/#categoricalcrossentropy-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Funcões Custo de Regressão\n",
    "\n",
    "Mais utilizadas\n",
    "* MSE - *Mean Squared Error* (Erro quadrado médio): [`tf.keras.losses.MeanSquaredError()`](https://keras.io/api/losses/regression_losses/#meansquarederror-class)\n",
    "* MAE - *Mean Absolute Error* (Erro absoluto médio): [`tf.keras.losses.MeanAbsoluteError()`](https://keras.io/api/losses/regression_losses/#meanabsoluteerror-class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Como construir sua rede neural no Keras\n",
    "\n",
    "Construir redes neurais com o **Keras** é muito fácil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exemplo de Classificação Binária"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()  # Cria uma rede neural sequencial - feed foward\n",
    "model.add(keras.layers.Dense(20, activation='relu', input_shape=(20, 1))) # primeira hidden layer\n",
    "model.add(keras.layers.Dense(20, activation='relu')) # segunda hidden layer\n",
    "model.add(keras.layers.Dense(1, activation= 'sigmoid')) # output layer com ativação sigmoid\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exemplo de Regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()  # Cria uma rede neural sequencial - feed foward\n",
    "model.add(keras.layers.Dense(20, activation='relu', input_shape=(20, 1))) # primeira hidden layer\n",
    "model.add(keras.layers.Dense(20, activation='relu')) # segunda hidden layer\n",
    "model.add(keras.layers.Dense(1, activation= 'linear')) # output layer com ativação linear\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Exemplo de Multiclassificação (não-binária - acima de duas classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()  # Cria uma rede neural sequencial - feed foward\n",
    "model.add(keras.layers.Dense(20, activation='relu', input_shape=(20, 1))) # primeira hidden layer\n",
    "model.add(keras.layers.Dense(20, activation='relu')) # segunda hidden layer\n",
    "model.add(keras.layers.Dense(10, activation= 'softmax')) # output layer com ativação softmax com 10 classes\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Como treinar sua rede neural no Keras\n",
    "\n",
    "Uma vez especificado o modelo, ele deve ser compilado com o comando `model.compile()` no qual deve se especificar a função custo `loss`, o otimizador `optimizer` e as métricas `metrics` como lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "             optimizer=keras.optimizers.SGD(),\n",
    "             metrics=[keras.metrics.Accuracy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Como ajustar o treinamento sua rede neural no Keras\n",
    "\n",
    "* Batch Size\n",
    "* Dropout\n",
    "* Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Batch Size\n",
    "\n",
    "Tamanho do Batch de dados que passa por vez pela rede neural antes da atualização dos parâmetros pelo *backpropagation*. Tamanhos grandes resultam em instabilidade no treinamento. Geralmente usam-se potências de $2$ $(2,4,8,16,\\dots, 2^n)$.\n",
    "\n",
    "Em Abril de 2018, Yann LeCun, um dos principais pesquisadores sobre redes neurais e ganhador do \"nobel\" da computação (Prêmio Turing) twittou em resposta à um artigo empírico que mostrava diversos contextos de *batch size*:\n",
    ">\"Friends don't let friends use mini-batches larger than 32\"\n",
    "\n",
    "Então 32 é um valor empiricamente verificado que dá estabilidade ao treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dropout\n",
    "\n",
    "Uma medida de regularização na qual evita-se overfitting proposta por Hinton em 2012. *Dropout* é um algoritmo que especifica que a cada iteração de época do treino os neurônios possuem uma probabilidade de serem removidos (não utilizados) para a aprendizagem. Geralmente a probabilidade ideal fica em torno de 20% ($0.2$).\n",
    "\n",
    "Coloca-se como se fosse uma camada após a camada que deseja aplicar o dropout\n",
    "\n",
    "```python\n",
    "model.add(keras.layers.Dense(4, activation='relu'))  # hidden layer\n",
    "model.add(keras.layers.Dropout(0.2))                 # dropout layer\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/dropout.gif\" alt=\"dropout\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### *Early Stopping* (Parada Precoce)\n",
    "\n",
    "Usar *Early Stopping* significa que, no final de cada época, devemos calcular o erro da rede. Se ele não diminuir significativamente após $n$ épocas, o treinamento é interrompido e a rede utiliza os parâmetros aprendidos até o ponto de interrupção.\n",
    "\n",
    "É um `callback` especificado durante o treino\n",
    "\n",
    "* [`tf.keras.callbacks.EarlyStopping()`](https://keras.io/api/callbacks/early_stopping/)\n",
    "    * `monitor='val_loss'` - métrica a monitorar\n",
    "    * `patience=N` - número de épocas a esperar até não conseguir nenhum aprimoramento para interrupção do treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Após isso é só treinar o modelo com `model.fit(X_train, y_train)` especificando o número de épocas `epochs`, tamanho do batch `batch_size` e dados de validação `validation_data`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "history  = model.fit(X_train, y_train,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     verbose=1,\n",
    "                     validation_data=(X_test, y_test),\n",
    "                     callbacks=[keras.callbacks.EarlyStopping(\n",
    "                         monitor='val_loss',\n",
    "                         patience=3)])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Visualizando o avanço do treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```python\n",
    "# Plot acurácia de treino e validação\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot custo de treino e validação\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exemplo com o dataset Titanic\n",
    "\n",
    "Contém 891 passageiros reais do Titanic que afundou em 15/04/1912 matando 1502 de 2224 passageiros e tripulação que estavam a bordo.\n",
    "\n",
    "* `survived`: *dummy* `0` ou `1` \n",
    "* `pclass`: Classe do Passageiro\n",
    "    - `1`: Primeira Classe\n",
    "    - `2`: Segunda Classe\n",
    "    - `3`: Terceira Classe\n",
    "* `sex`: Sexo `male` ou `female`\n",
    "* `age`: Idade\n",
    "* `sibsp`: Número de Irmãos (*Siblings*) e Esposas (*spouse*) a bordo\n",
    "* `parch`: Número de pais/filhos a bordo\n",
    "* `fare`: Valor pago pela passagem em libras\n",
    "* `embarked`: Porto que embarcou\n",
    "    - `C`: Cherbourg\n",
    "    - `Q`: Queenstown\n",
    "    - `S`: Southampton)\n",
    "* `class`: Mesmo que `pclass` só que em texto\n",
    "* `adult_male`: *dummy* para `age > 16` e `sex == 'male'`\n",
    "* `deck`: Qual deck a cabine do passageiro se situava\n",
    "* `alive`: Mesmo que survived só que com `yes` ou `no`\n",
    "* `alone`: *dummy* para se viajava sozinho\n",
    "\n",
    ">Obs: usar `random_state = 123`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/titanic.png\" alt=\"titanic\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "feature_names = ['pclass', 'female', 'age', 'fare']\n",
    "titanic['female'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
    "titanic.dropna(subset=feature_names, inplace=True)  #891 para 714\n",
    "\n",
    "X = titanic[feature_names].to_numpy()\n",
    "y = titanic['survived'].to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.25,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print('Tamanho de X_train: ', X_train.shape)\n",
    "print('Tamanho de X_test: ', X_test.shape)\n",
    "print('Tamanho de y_train: ', y_train.shape)\n",
    "print('Tamanho de y_test: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()  # Cria uma rede neural sequencial - feed foward\n",
    "model.add(keras.layers.Dense(4, activation='relu', input_shape=(4, ))) # primeira hidden layer\n",
    "model.add(keras.layers.Dropout(0.2))\n",
    "model.add(keras.layers.Dense(1, activation= 'sigmoid')) # output layer com ativação sigmoid\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.binary_crossentropy,\n",
    "             optimizer=keras.optimizers.Adam(),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32  # X_train 535 / 32 = 16.71 (então são 17 batches de 32)\n",
    "epochs = 100\n",
    "\n",
    "history  = model.fit(X_train, y_train,\n",
    "                     batch_size=batch_size,\n",
    "                     epochs=epochs,\n",
    "                     verbose=1,\n",
    "                     validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot acurácia de treino e validação\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot custo de treino e validação\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Acurácia do Modelo\n",
    "\n",
    "Usar o comando `model.evaluate()`\n",
    "\n",
    "Para a métrica acurácia, retorna um score de acurácia `float` entre $0$ e $1$\n",
    "    \n",
    "> Obs: Regressão Logística acurácias: 0.69 Treino e 0.7 Teste\n",
    "\n",
    "> Obs: *Support Vector Machines* acurácias: 0.79 Treino e 0.75 Teste\n",
    "\n",
    "> Obs: Árvores de Decisão acurácias: 0.79 Treino e 0.79 Teste\n",
    "\n",
    "> Obs: Florestas Aleatórias acurácias: 0.84 Treino e 0.82 Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "# X_test 179 / 32 = 5.59 (então são 6 batches de 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fazer previsões com o modelo\n",
    "\n",
    "* `model.predict(X_new)` para obter uma probabilidade `float` entre $0$ e $1$\n",
    "* `(model.predict(X_new) > 0.5).astype(\"int32\")` para obter um `int` $0$ ou $1$ representando a classe (Classificação Binária)\n",
    "* `np.argmax(model.predict(X_new), axis=-1)` para obter um `int` representando a classe (Classificação Multiclasse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "(model.predict(X_test) > 0.5).astype(\"int32\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
